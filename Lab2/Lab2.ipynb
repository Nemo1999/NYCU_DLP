{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import read_bci_data\n",
    "from  torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torch import tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = {\n",
    "    'bs': 64,\n",
    "    'lr':1e-2,\n",
    "    'epochs':500,\n",
    "    'optimizer': 'sgd', # optimizer  \n",
    "    'NN':'EEGNet',\n",
    "    'act':'ELU'\n",
    "}\n",
    "\n",
    "def hp_2_str(hp):\n",
    "    return f\"{hp['NN']}|{hp['act']}|lr={hp['lr']}|opt={hp['optimizer']}|ep={hp['epochs']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_loader(hp):\n",
    "    batch_size = hp['bs']\n",
    "    #read data from file\n",
    "    train_x, train_y, test_x, test_y =  read_bci_data()\n",
    "    # Dataloader \n",
    "    train_dl = DataLoader(TensorDataset(tensor(train_x,dtype=torch.float), tensor(train_y,dtype=torch.long)),         batch_size=batch_size, shuffle= True)\n",
    "    test_dl = DataLoader(TensorDataset(tensor(test_x,dtype=torch.float), tensor(test_y,dtype=torch.long)),            batch_size=batch_size, shuffle = False)\n",
    "    return train_dl, test_dl\n",
    "\n",
    "def get_device():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"Using {} device\".format(device))\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation(act_name):\n",
    "    if act_name == 'ELU':\n",
    "        return nn.ELU()\n",
    "    elif act_name == 'ReLU':\n",
    "        return nn.ReLU()\n",
    "    else:\n",
    "        return nn.LeakyReLU()\n",
    "\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self,hp):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.firstConv = nn.Sequential(\n",
    "            nn.Conv2d(1,16,kernel_size=(1,51), stride=(1,1), padding=(0,25), bias=False),\n",
    "            nn.BatchNorm2d(16,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        )\n",
    "        self.depthwiseConv = nn.Sequential(\n",
    "            nn.Conv2d(16,32, kernel_size=(2,1), stride=(1,1), groups=16, bias=False),\n",
    "            nn.BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            get_activation(hp['act']),\n",
    "            nn.AvgPool2d(kernel_size=(1,4), stride=(1,4), padding=0),\n",
    "            nn.Dropout(p=0.25)\n",
    "        )\n",
    "        self.separableConv = nn.Sequential(\n",
    "            nn.Conv2d(32,32,kernel_size=(1,15), stride=(1,1), padding=(0,7), bias=False),\n",
    "            nn.BatchNorm2d(32,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            get_activation(hp['act']),\n",
    "            nn.AvgPool2d(kernel_size=(1,8), stride=(1,8),padding=0),\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.classify = nn.Sequential(\n",
    "            nn.Linear(32*23, 2, bias=False)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = self.firstConv(x)\n",
    "        x = self.depthwiseConv(x)\n",
    "        x = self.separableConv(x)\n",
    "        x = self.classify(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(hp):\n",
    "    return nn.CrossEntropyLoss()\n",
    "\n",
    "def get_optimizer(hp,model):\n",
    "    return torch.optim.SGD(model.parameters(),lr=hp['lr'])\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer, device):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    # turn model into training mode\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    correct = 0.0\n",
    "    for batch, (X,y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred,y)\n",
    "        epoch_loss += loss\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \"\"\"\n",
    "        if batch %10 == 0:\n",
    "            loss, current = loss.item(), batch*len(X)\n",
    "            print(f\" train loss: {loss:>7f} \\t\\t [{current:>5d}/{size:>5d}]\")\n",
    "        \"\"\"\n",
    "    epoch_loss /= num_batches\n",
    "    correct/=size\n",
    "    #print(f\"epoch loss: {epoch_loss:>7f}\")\n",
    "    return correct, epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn, device):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    # turn model into eval mode\n",
    "    model.eval()\n",
    "    test_loss, correct = 0,0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            #print(y.dtype)\n",
    "            pred = model(X)\n",
    "            #print(pred.dtype)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    #print(f'Test Accuracy: {(100 * correct):>0.1f}%, test loss:{test_loss:>8f} \\n')\n",
    "    return correct, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_with_hp(hp,save_result=False):\n",
    "    \n",
    "   \n",
    "    # get dataloader\n",
    "    train_dl, test_dl = gen_loader(hp)\n",
    "    # get device\n",
    "    device = get_device()\n",
    "    # generate model\n",
    "    if hp['NN'] == \"EEGNet\":\n",
    "        model = EEGNet(hp).to(device)\n",
    "    else:\n",
    "        model = DeepCNN(hp).to(device)\n",
    "    print(model)\n",
    "    \n",
    "    loss_fn = get_loss(hp)\n",
    "    optimizer = get_optimizer(hp,model)\n",
    "\n",
    "    test_acc_history = []\n",
    "    test_loss_history = []\n",
    "    train_acc_history = []\n",
    "    train_loss_history = []\n",
    "    \n",
    "    epochs = hp['epochs']\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}  \",end='')\n",
    "        acc, loss = train(train_dl, model, loss_fn, optimizer,device)\n",
    "        train_acc_history.append(acc)\n",
    "        train_loss_history.append(loss)\n",
    "        print(f\"train_acc: {acc:<5.4f}  \",end='')\n",
    "\n",
    "        acc, loss = test(test_dl, model, loss_fn, device)\n",
    "        test_acc_history.append(acc)\n",
    "        test_loss_history.append(loss)\n",
    "        print(f\"test_acc: {acc:<5.4f}  \",end='')\n",
    "\n",
    "        print('\\n',end='')\n",
    "\n",
    "    # save model parameters\n",
    "    if save_result:\n",
    "        pth_name = 'checkpoints/'+hp_2_str(hp)+'model.pth'\n",
    "        torch.save(model.state_dict(), pth_name)\n",
    "        print(f\"Save PyTorch Model State to {pth_name}\")\n",
    "\n",
    "        npz_name = 'learning_curves/'+hp_2_str(hp)+'curves.npz'\n",
    "        np.savez_compressed(\n",
    "            npz_name,\n",
    "            train_acc=np.array(train_acc_history),\n",
    "            train_loss=np.array(train_loss_history),\n",
    "            test_acc_hisory=np.array(test_acc_history),\n",
    "            test_loss_history=np.array(test_loss_history)\n",
    "        )\n",
    "        print(f\"Save learning curves to {npz_name}\")\n",
    "    print(\"Done!\")\n",
    "    history = {\n",
    "        'train_acc' : np.array(train_acc_history),\n",
    "        'train_loss' : np.array(train_loss_history),\n",
    "        'test_acc_hisory' : np.array(test_acc_history),\n",
    "        'test_loss_history' : np.array(test_loss_history)\n",
    "\n",
    "    }\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.8241  test_acc: 0.7491  \n",
      "Epoch 297  train_acc: 0.8083  test_acc: 0.7481  \n",
      "Epoch 298  train_acc: 0.8083  test_acc: 0.7500  \n",
      "Epoch 299  train_acc: 0.8130  test_acc: 0.7519  \n",
      "Epoch 300  train_acc: 0.8306  test_acc: 0.7481  \n",
      "Epoch 301  train_acc: 0.8148  test_acc: 0.7500  \n",
      "Epoch 302  train_acc: 0.8148  test_acc: 0.7528  \n",
      "Epoch 303  train_acc: 0.8157  test_acc: 0.7519  \n",
      "Epoch 304  train_acc: 0.8130  test_acc: 0.7500  \n",
      "Epoch 305  train_acc: 0.8120  test_acc: 0.7509  \n",
      "Epoch 306  train_acc: 0.8083  test_acc: 0.7528  \n",
      "Epoch 307  train_acc: 0.8130  test_acc: 0.7491  \n",
      "Epoch 308  train_acc: 0.8204  test_acc: 0.7556  \n",
      "Epoch 309  train_acc: 0.8120  test_acc: 0.7519  \n",
      "Epoch 310  train_acc: 0.8102  test_acc: 0.7537  \n",
      "Epoch 311  train_acc: 0.8167  test_acc: 0.7491  \n",
      "Epoch 312  train_acc: 0.8315  test_acc: 0.7481  \n",
      "Epoch 313  train_acc: 0.8222  test_acc: 0.7528  \n",
      "Epoch 314  train_acc: 0.8083  test_acc: 0.7491  \n",
      "Epoch 315  train_acc: 0.8185  test_acc: 0.7481  \n",
      "Epoch 316  train_acc: 0.8185  test_acc: 0.7491  \n",
      "Epoch 317  train_acc: 0.8130  test_acc: 0.7491  \n",
      "Epoch 318  train_acc: 0.8167  test_acc: 0.7463  \n",
      "Epoch 319  train_acc: 0.8278  test_acc: 0.7528  \n",
      "Epoch 320  train_acc: 0.8111  test_acc: 0.7519  \n",
      "Epoch 321  train_acc: 0.8083  test_acc: 0.7509  \n",
      "Epoch 322  train_acc: 0.8130  test_acc: 0.7519  \n",
      "Epoch 323  train_acc: 0.8204  test_acc: 0.7509  \n",
      "Epoch 324  train_acc: 0.8111  test_acc: 0.7556  \n",
      "Epoch 325  train_acc: 0.8167  test_acc: 0.7519  \n",
      "Epoch 326  train_acc: 0.8157  test_acc: 0.7528  \n",
      "Epoch 327  train_acc: 0.8194  test_acc: 0.7528  \n",
      "Epoch 328  train_acc: 0.8194  test_acc: 0.7574  \n",
      "Epoch 329  train_acc: 0.8176  test_acc: 0.7556  \n",
      "Epoch 330  train_acc: 0.8296  test_acc: 0.7565  \n",
      "Epoch 331  train_acc: 0.8120  test_acc: 0.7565  \n",
      "Epoch 332  train_acc: 0.8176  test_acc: 0.7537  \n",
      "Epoch 333  train_acc: 0.8398  test_acc: 0.7565  \n",
      "Epoch 334  train_acc: 0.8157  test_acc: 0.7546  \n",
      "Epoch 335  train_acc: 0.8102  test_acc: 0.7565  \n",
      "Epoch 336  train_acc: 0.8204  test_acc: 0.7593  \n",
      "Epoch 337  train_acc: 0.8046  test_acc: 0.7574  \n",
      "Epoch 338  train_acc: 0.8352  test_acc: 0.7574  \n",
      "Epoch 339  train_acc: 0.8176  test_acc: 0.7556  \n",
      "Epoch 340  train_acc: 0.8148  test_acc: 0.7574  \n",
      "Epoch 341  train_acc: 0.8194  test_acc: 0.7583  \n",
      "Epoch 342  train_acc: 0.8278  test_acc: 0.7556  \n",
      "Epoch 343  train_acc: 0.8148  test_acc: 0.7583  \n",
      "Epoch 344  train_acc: 0.8222  test_acc: 0.7556  \n",
      "Epoch 345  train_acc: 0.8296  test_acc: 0.7574  \n",
      "Epoch 346  train_acc: 0.8102  test_acc: 0.7565  \n",
      "Epoch 347  train_acc: 0.8157  test_acc: 0.7556  \n",
      "Epoch 348  train_acc: 0.8269  test_acc: 0.7546  \n",
      "Epoch 349  train_acc: 0.8231  test_acc: 0.7556  \n",
      "Epoch 350  train_acc: 0.8269  test_acc: 0.7593  \n",
      "Epoch 351  train_acc: 0.8315  test_acc: 0.7574  \n",
      "Epoch 352  train_acc: 0.8194  test_acc: 0.7574  \n",
      "Epoch 353  train_acc: 0.8287  test_acc: 0.7574  \n",
      "Epoch 354  train_acc: 0.8194  test_acc: 0.7574  \n",
      "Epoch 355  train_acc: 0.8185  test_acc: 0.7593  \n",
      "Epoch 356  train_acc: 0.8241  test_acc: 0.7574  \n",
      "Epoch 357  train_acc: 0.8324  test_acc: 0.7565  \n",
      "Epoch 358  train_acc: 0.8435  test_acc: 0.7574  \n",
      "Epoch 359  train_acc: 0.8185  test_acc: 0.7556  \n",
      "Epoch 360  train_acc: 0.8093  test_acc: 0.7556  \n",
      "Epoch 361  train_acc: 0.8139  test_acc: 0.7565  \n",
      "Epoch 362  train_acc: 0.8306  test_acc: 0.7574  \n",
      "Epoch 363  train_acc: 0.8324  test_acc: 0.7593  \n",
      "Epoch 364  train_acc: 0.8120  test_acc: 0.7565  \n",
      "Epoch 365  train_acc: 0.8306  test_acc: 0.7556  \n",
      "Epoch 366  train_acc: 0.8185  test_acc: 0.7583  \n",
      "Epoch 367  train_acc: 0.8380  test_acc: 0.7583  \n",
      "Epoch 368  train_acc: 0.8380  test_acc: 0.7565  \n",
      "Epoch 369  train_acc: 0.8287  test_acc: 0.7546  \n",
      "Epoch 370  train_acc: 0.8278  test_acc: 0.7583  \n",
      "Epoch 371  train_acc: 0.8278  test_acc: 0.7602  \n",
      "Epoch 372  train_acc: 0.8324  test_acc: 0.7593  \n",
      "Epoch 373  train_acc: 0.8241  test_acc: 0.7602  \n",
      "Epoch 374  train_acc: 0.8231  test_acc: 0.7574  \n",
      "Epoch 375  train_acc: 0.8269  test_acc: 0.7620  \n",
      "Epoch 376  train_acc: 0.8259  test_acc: 0.7611  \n",
      "Epoch 377  train_acc: 0.8324  test_acc: 0.7611  \n",
      "Epoch 378  train_acc: 0.8231  test_acc: 0.7565  \n",
      "Epoch 379  train_acc: 0.8213  test_acc: 0.7620  \n",
      "Epoch 380  train_acc: 0.8370  test_acc: 0.7602  \n",
      "Epoch 381  train_acc: 0.8333  test_acc: 0.7583  \n",
      "Epoch 382  train_acc: 0.8389  test_acc: 0.7602  \n",
      "Epoch 383  train_acc: 0.8306  test_acc: 0.7620  \n",
      "Epoch 384  train_acc: 0.8259  test_acc: 0.7574  \n",
      "Epoch 385  train_acc: 0.8343  test_acc: 0.7611  \n",
      "Epoch 386  train_acc: 0.8241  test_acc: 0.7583  \n",
      "Epoch 387  train_acc: 0.8148  test_acc: 0.7583  \n",
      "Epoch 388  train_acc: 0.8241  test_acc: 0.7593  \n",
      "Epoch 389  train_acc: 0.8296  test_acc: 0.7611  \n",
      "Epoch 390  train_acc: 0.8241  test_acc: 0.7611  \n",
      "Epoch 391  train_acc: 0.8417  test_acc: 0.7602  \n",
      "Epoch 392  train_acc: 0.8343  test_acc: 0.7574  \n",
      "Epoch 393  train_acc: 0.8398  test_acc: 0.7611  \n",
      "Epoch 394  train_acc: 0.8324  test_acc: 0.7583  \n",
      "Epoch 395  train_acc: 0.8324  test_acc: 0.7574  \n",
      "Epoch 396  train_acc: 0.8259  test_acc: 0.7620  \n",
      "Epoch 397  train_acc: 0.8213  test_acc: 0.7630  \n",
      "Epoch 398  train_acc: 0.8333  test_acc: 0.7583  \n",
      "Epoch 399  train_acc: 0.8426  test_acc: 0.7593  \n",
      "Epoch 400  train_acc: 0.8222  test_acc: 0.7602  \n",
      "Epoch 401  train_acc: 0.8435  test_acc: 0.7639  \n",
      "Epoch 402  train_acc: 0.8407  test_acc: 0.7574  \n",
      "Epoch 403  train_acc: 0.8296  test_acc: 0.7593  \n",
      "Epoch 404  train_acc: 0.8241  test_acc: 0.7593  \n",
      "Epoch 405  train_acc: 0.8324  test_acc: 0.7556  \n",
      "Epoch 406  train_acc: 0.8241  test_acc: 0.7620  \n",
      "Epoch 407  train_acc: 0.8269  test_acc: 0.7565  \n",
      "Epoch 408  train_acc: 0.8343  test_acc: 0.7602  \n",
      "Epoch 409  train_acc: 0.8500  test_acc: 0.7593  \n",
      "Epoch 410  train_acc: 0.8417  test_acc: 0.7630  \n",
      "Epoch 411  train_acc: 0.8352  test_acc: 0.7593  \n",
      "Epoch 412  train_acc: 0.8370  test_acc: 0.7639  \n",
      "Epoch 413  train_acc: 0.8435  test_acc: 0.7620  \n",
      "Epoch 414  train_acc: 0.8120  test_acc: 0.7602  \n",
      "Epoch 415  train_acc: 0.8380  test_acc: 0.7583  \n",
      "Epoch 416  train_acc: 0.8454  test_acc: 0.7611  \n",
      "Epoch 417  train_acc: 0.8417  test_acc: 0.7602  \n",
      "Epoch 418  train_acc: 0.8426  test_acc: 0.7611  \n",
      "Epoch 419  train_acc: 0.8333  test_acc: 0.7593  \n",
      "Epoch 420  train_acc: 0.8287  test_acc: 0.7648  \n",
      "Epoch 421  train_acc: 0.8491  test_acc: 0.7620  \n",
      "Epoch 422  train_acc: 0.8352  test_acc: 0.7630  \n",
      "Epoch 423  train_acc: 0.8380  test_acc: 0.7639  \n",
      "Epoch 424  train_acc: 0.8398  test_acc: 0.7620  \n",
      "Epoch 425  train_acc: 0.8324  test_acc: 0.7602  \n",
      "Epoch 426  train_acc: 0.8370  test_acc: 0.7667  \n",
      "Epoch 427  train_acc: 0.8352  test_acc: 0.7565  \n",
      "Epoch 428  train_acc: 0.8380  test_acc: 0.7593  \n",
      "Epoch 429  train_acc: 0.8519  test_acc: 0.7583  \n",
      "Epoch 430  train_acc: 0.8380  test_acc: 0.7574  \n",
      "Epoch 431  train_acc: 0.8380  test_acc: 0.7593  \n",
      "Epoch 432  train_acc: 0.8463  test_acc: 0.7574  \n",
      "Epoch 433  train_acc: 0.8250  test_acc: 0.7620  \n",
      "Epoch 434  train_acc: 0.8528  test_acc: 0.7602  \n",
      "Epoch 435  train_acc: 0.8259  test_acc: 0.7620  \n",
      "Epoch 436  train_acc: 0.8278  test_acc: 0.7620  \n",
      "Epoch 437  train_acc: 0.8259  test_acc: 0.7583  \n",
      "Epoch 438  train_acc: 0.8454  test_acc: 0.7565  \n",
      "Epoch 439  train_acc: 0.8417  test_acc: 0.7546  \n",
      "Epoch 440  train_acc: 0.8528  test_acc: 0.7611  \n",
      "Epoch 441  train_acc: 0.8417  test_acc: 0.7565  \n",
      "Epoch 442  train_acc: 0.8296  test_acc: 0.7620  \n",
      "Epoch 443  train_acc: 0.8259  test_acc: 0.7565  \n",
      "Epoch 444  train_acc: 0.8380  test_acc: 0.7602  \n",
      "Epoch 445  train_acc: 0.8426  test_acc: 0.7620  \n",
      "Epoch 446  train_acc: 0.8343  test_acc: 0.7630  \n",
      "Epoch 447  train_acc: 0.8315  test_acc: 0.7648  \n",
      "Epoch 448  train_acc: 0.8370  test_acc: 0.7583  \n",
      "Epoch 449  train_acc: 0.8407  test_acc: 0.7593  \n",
      "Epoch 450  train_acc: 0.8352  test_acc: 0.7611  \n",
      "Epoch 451  train_acc: 0.8389  test_acc: 0.7602  \n",
      "Epoch 452  train_acc: 0.8361  test_acc: 0.7583  \n",
      "Epoch 453  train_acc: 0.8213  test_acc: 0.7648  \n",
      "Epoch 454  train_acc: 0.8287  test_acc: 0.7602  \n",
      "Epoch 455  train_acc: 0.8472  test_acc: 0.7639  \n",
      "Epoch 456  train_acc: 0.8352  test_acc: 0.7630  \n",
      "Epoch 457  train_acc: 0.8454  test_acc: 0.7602  \n",
      "Epoch 458  train_acc: 0.8491  test_acc: 0.7620  \n",
      "Epoch 459  train_acc: 0.8352  test_acc: 0.7593  \n",
      "Epoch 460  train_acc: 0.8407  test_acc: 0.7630  \n",
      "Epoch 461  train_acc: 0.8472  test_acc: 0.7593  \n",
      "Epoch 462  train_acc: 0.8444  test_acc: 0.7639  \n",
      "Epoch 463  train_acc: 0.8454  test_acc: 0.7648  \n",
      "Epoch 464  train_acc: 0.8417  test_acc: 0.7620  \n",
      "Epoch 465  train_acc: 0.8435  test_acc: 0.7639  \n",
      "Epoch 466  train_acc: 0.8352  test_acc: 0.7667  \n",
      "Epoch 467  train_acc: 0.8454  test_acc: 0.7648  \n",
      "Epoch 468  train_acc: 0.8343  test_acc: 0.7630  \n",
      "Epoch 469  train_acc: 0.8454  test_acc: 0.7648  \n",
      "Epoch 470  train_acc: 0.8472  test_acc: 0.7639  \n",
      "Epoch 471  train_acc: 0.8509  test_acc: 0.7648  \n",
      "Epoch 472  train_acc: 0.8370  test_acc: 0.7611  \n",
      "Epoch 473  train_acc: 0.8491  test_acc: 0.7611  \n",
      "Epoch 474  train_acc: 0.8417  test_acc: 0.7657  \n",
      "Epoch 475  train_acc: 0.8454  test_acc: 0.7630  \n",
      "Epoch 476  train_acc: 0.8444  test_acc: 0.7639  \n",
      "Epoch 477  train_acc: 0.8435  test_acc: 0.7648  \n",
      "Epoch 478  train_acc: 0.8556  test_acc: 0.7657  \n",
      "Epoch 479  train_acc: 0.8324  test_acc: 0.7583  \n",
      "Epoch 480  train_acc: 0.8491  test_acc: 0.7630  \n",
      "Epoch 481  train_acc: 0.8343  test_acc: 0.7620  \n",
      "Epoch 482  train_acc: 0.8519  test_acc: 0.7639  \n",
      "Epoch 483  train_acc: 0.8454  test_acc: 0.7657  \n",
      "Epoch 484  train_acc: 0.8463  test_acc: 0.7648  \n",
      "Epoch 485  train_acc: 0.8454  test_acc: 0.7639  \n",
      "Epoch 486  train_acc: 0.8417  test_acc: 0.7620  \n",
      "Epoch 487  train_acc: 0.8537  test_acc: 0.7639  \n",
      "Epoch 488  train_acc: 0.8500  test_acc: 0.7630  \n",
      "Epoch 489  train_acc: 0.8500  test_acc: 0.7630  \n",
      "Epoch 490  train_acc: 0.8398  test_acc: 0.7620  \n",
      "Epoch 491  train_acc: 0.8324  test_acc: 0.7648  \n",
      "Epoch 492  train_acc: 0.8472  test_acc: 0.7620  \n",
      "Epoch 493  train_acc: 0.8519  test_acc: 0.7639  \n",
      "Epoch 494  train_acc: 0.8463  test_acc: 0.7630  \n",
      "Epoch 495  train_acc: 0.8380  test_acc: 0.7639  \n",
      "Epoch 496  train_acc: 0.8537  test_acc: 0.7667  \n",
      "Epoch 497  train_acc: 0.8519  test_acc: 0.7639  \n",
      "Epoch 498  train_acc: 0.8602  test_acc: 0.7648  \n",
      "Epoch 499  train_acc: 0.8546  test_acc: 0.7676  \n",
      "Epoch 500  train_acc: 0.8500  test_acc: 0.7667  \n",
      "Epoch 501  train_acc: 0.8611  test_acc: 0.7639  \n",
      "Epoch 502  train_acc: 0.8630  test_acc: 0.7639  \n",
      "Epoch 503  train_acc: 0.8481  test_acc: 0.7648  \n",
      "Epoch 504  train_acc: 0.8565  test_acc: 0.7648  \n",
      "Epoch 505  train_acc: 0.8593  test_acc: 0.7639  \n",
      "Epoch 506  train_acc: 0.8454  test_acc: 0.7639  \n",
      "Epoch 507  train_acc: 0.8380  test_acc: 0.7630  \n",
      "Epoch 508  train_acc: 0.8565  test_acc: 0.7639  \n",
      "Epoch 509  train_acc: 0.8343  test_acc: 0.7648  \n",
      "Epoch 510  train_acc: 0.8444  test_acc: 0.7667  \n",
      "Epoch 511  train_acc: 0.8435  test_acc: 0.7676  \n",
      "Epoch 512  train_acc: 0.8481  test_acc: 0.7611  \n",
      "Epoch 513  train_acc: 0.8417  test_acc: 0.7694  \n",
      "Epoch 514  train_acc: 0.8500  test_acc: 0.7676  \n",
      "Epoch 515  train_acc: 0.8556  test_acc: 0.7657  \n",
      "Epoch 516  train_acc: 0.8537  test_acc: 0.7648  \n",
      "Epoch 517  train_acc: 0.8556  test_acc: 0.7639  \n",
      "Epoch 518  train_acc: 0.8546  test_acc: 0.7667  \n",
      "Epoch 519  train_acc: 0.8611  test_acc: 0.7639  \n",
      "Epoch 520  train_acc: 0.8519  test_acc: 0.7657  \n",
      "Epoch 521  train_acc: 0.8620  test_acc: 0.7648  \n",
      "Epoch 522  train_acc: 0.8574  test_acc: 0.7648  \n",
      "Epoch 523  train_acc: 0.8454  test_acc: 0.7639  \n",
      "Epoch 524  train_acc: 0.8519  test_acc: 0.7602  \n",
      "Epoch 525  train_acc: 0.8556  test_acc: 0.7667  \n",
      "Epoch 526  train_acc: 0.8639  test_acc: 0.7630  \n",
      "Epoch 527  train_acc: 0.8491  test_acc: 0.7648  \n",
      "Epoch 528  train_acc: 0.8472  test_acc: 0.7639  \n",
      "Epoch 529  train_acc: 0.8472  test_acc: 0.7648  \n",
      "Epoch 530  train_acc: 0.8713  test_acc: 0.7685  \n",
      "Epoch 531  train_acc: 0.8565  test_acc: 0.7685  \n",
      "Epoch 532  train_acc: 0.8602  test_acc: 0.7657  \n",
      "Epoch 533  train_acc: 0.8593  test_acc: 0.7574  \n",
      "Epoch 534  train_acc: 0.8574  test_acc: 0.7667  \n",
      "Epoch 535  train_acc: 0.8509  test_acc: 0.7676  \n",
      "Epoch 536  train_acc: 0.8500  test_acc: 0.7657  \n",
      "Epoch 537  train_acc: 0.8556  test_acc: 0.7667  \n",
      "Epoch 538  train_acc: 0.8667  test_acc: 0.7704  \n",
      "Epoch 539  train_acc: 0.8602  test_acc: 0.7704  \n",
      "Epoch 540  train_acc: 0.8398  test_acc: 0.7657  \n",
      "Epoch 541  train_acc: 0.8565  test_acc: 0.7657  \n",
      "Epoch 542  train_acc: 0.8620  test_acc: 0.7685  \n",
      "Epoch 543  train_acc: 0.8491  test_acc: 0.7667  \n",
      "Epoch 544  train_acc: 0.8565  test_acc: 0.7685  \n",
      "Epoch 545  train_acc: 0.8704  test_acc: 0.7685  \n",
      "Epoch 546  train_acc: 0.8417  test_acc: 0.7667  \n",
      "Epoch 547  train_acc: 0.8657  test_acc: 0.7657  \n",
      "Epoch 548  train_acc: 0.8648  test_acc: 0.7620  \n",
      "Epoch 549  train_acc: 0.8620  test_acc: 0.7611  \n",
      "Epoch 550  train_acc: 0.8593  test_acc: 0.7648  \n",
      "Epoch 551  train_acc: 0.8491  test_acc: 0.7685  \n",
      "Epoch 552  train_acc: 0.8546  test_acc: 0.7694  \n",
      "Epoch 553  train_acc: 0.8750  test_acc: 0.7685  \n",
      "Epoch 554  train_acc: 0.8648  test_acc: 0.7667  \n",
      "Epoch 555  train_acc: 0.8565  test_acc: 0.7676  \n",
      "Epoch 556  train_acc: 0.8685  test_acc: 0.7676  \n",
      "Epoch 557  train_acc: 0.8528  test_acc: 0.7648  \n",
      "Epoch 558  train_acc: 0.8583  test_acc: 0.7676  \n",
      "Epoch 559  train_acc: 0.8593  test_acc: 0.7648  \n",
      "Epoch 560  train_acc: 0.8602  test_acc: 0.7657  \n",
      "Epoch 561  train_acc: 0.8704  test_acc: 0.7685  \n",
      "Epoch 562  train_acc: 0.8519  test_acc: 0.7704  \n",
      "Epoch 563  train_acc: 0.8620  test_acc: 0.7713  \n",
      "Epoch 564  train_acc: 0.8537  test_acc: 0.7704  \n",
      "Epoch 565  train_acc: 0.8750  test_acc: 0.7676  \n",
      "Epoch 566  train_acc: 0.8565  test_acc: 0.7694  \n",
      "Epoch 567  train_acc: 0.8657  test_acc: 0.7667  \n",
      "Epoch 568  train_acc: 0.8694  test_acc: 0.7676  \n",
      "Epoch 569  train_acc: 0.8593  test_acc: 0.7704  \n",
      "Epoch 570  train_acc: 0.8315  test_acc: 0.7657  \n",
      "Epoch 571  train_acc: 0.8769  test_acc: 0.7667  \n",
      "Epoch 572  train_acc: 0.8815  test_acc: 0.7685  \n",
      "Epoch 573  train_acc: 0.8676  test_acc: 0.7704  \n",
      "Epoch 574  train_acc: 0.8639  test_acc: 0.7704  \n",
      "Epoch 575  train_acc: 0.8593  test_acc: 0.7685  \n",
      "Epoch 576  train_acc: 0.8565  test_acc: 0.7676  \n",
      "Epoch 577  train_acc: 0.8611  test_acc: 0.7694  \n",
      "Epoch 578  train_acc: 0.8722  test_acc: 0.7704  \n",
      "Epoch 579  train_acc: 0.8565  test_acc: 0.7750  \n",
      "Epoch 580  train_acc: 0.8769  test_acc: 0.7731  \n",
      "Epoch 581  train_acc: 0.8685  test_acc: 0.7694  \n",
      "Epoch 582  train_acc: 0.8759  test_acc: 0.7713  \n",
      "Epoch 583  train_acc: 0.8741  test_acc: 0.7694  \n",
      "Epoch 584  train_acc: 0.8565  test_acc: 0.7704  \n",
      "Epoch 585  train_acc: 0.8583  test_acc: 0.7722  \n",
      "Epoch 586  train_acc: 0.8676  test_acc: 0.7694  \n",
      "Epoch 587  train_acc: 0.8620  test_acc: 0.7741  \n",
      "Epoch 588  train_acc: 0.8806  test_acc: 0.7731  \n",
      "Epoch 589  train_acc: 0.8806  test_acc: 0.7741  \n",
      "Epoch 590  train_acc: 0.8759  test_acc: 0.7704  \n",
      "Epoch 591  train_acc: 0.8870  test_acc: 0.7741  \n",
      "Epoch 592  train_acc: 0.8741  test_acc: 0.7750  \n",
      "Epoch 593  train_acc: 0.8611  test_acc: 0.7769  \n",
      "Epoch 594  train_acc: 0.8713  test_acc: 0.7741  \n",
      "Epoch 595  train_acc: 0.8648  test_acc: 0.7769  \n",
      "Epoch 596  train_acc: 0.8611  test_acc: 0.7731  \n",
      "Epoch 597  train_acc: 0.8435  test_acc: 0.7759  \n",
      "Epoch 598  train_acc: 0.8722  test_acc: 0.7778  \n",
      "Epoch 599  train_acc: 0.8796  test_acc: 0.7796  \n",
      "Epoch 600  train_acc: 0.8741  test_acc: 0.7778  \n",
      "Epoch 601  train_acc: 0.8824  test_acc: 0.7796  \n",
      "Epoch 602  train_acc: 0.8611  test_acc: 0.7769  \n",
      "Epoch 603  train_acc: 0.8833  test_acc: 0.7852  \n",
      "Epoch 604  train_acc: 0.8676  test_acc: 0.7759  \n",
      "Epoch 605  train_acc: 0.8796  test_acc: 0.7704  \n",
      "Epoch 606  train_acc: 0.8704  test_acc: 0.7704  \n",
      "Epoch 607  train_acc: 0.8694  test_acc: 0.7759  \n",
      "Epoch 608  train_acc: 0.8759  test_acc: 0.7815  \n",
      "Epoch 609  train_acc: 0.8694  test_acc: 0.7796  \n",
      "Epoch 610  train_acc: 0.8769  test_acc: 0.7778  \n",
      "Epoch 611  train_acc: 0.8676  test_acc: 0.7769  \n",
      "Epoch 612  train_acc: 0.8907  test_acc: 0.7731  \n",
      "Epoch 613  train_acc: 0.8769  test_acc: 0.7750  \n",
      "Epoch 614  train_acc: 0.8731  test_acc: 0.7843  \n",
      "Epoch 615  train_acc: 0.8731  test_acc: 0.7861  \n",
      "Epoch 616  train_acc: 0.8685  test_acc: 0.7778  \n",
      "Epoch 617  train_acc: 0.8722  test_acc: 0.7769  \n",
      "Epoch 618  train_acc: 0.8935  test_acc: 0.7806  \n",
      "Epoch 619  train_acc: 0.8806  test_acc: 0.7759  \n",
      "Epoch 620  train_acc: 0.8731  test_acc: 0.7806  \n",
      "Epoch 621  train_acc: 0.8676  test_acc: 0.7815  \n",
      "Epoch 622  train_acc: 0.8713  test_acc: 0.7833  \n",
      "Epoch 623  train_acc: 0.8722  test_acc: 0.7843  \n",
      "Epoch 624  train_acc: 0.8722  test_acc: 0.7843  \n",
      "Epoch 625  train_acc: 0.8935  test_acc: 0.7815  \n",
      "Epoch 626  train_acc: 0.8880  test_acc: 0.7824  \n",
      "Epoch 627  train_acc: 0.8778  test_acc: 0.7833  \n",
      "Epoch 628  train_acc: 0.8843  test_acc: 0.7843  \n",
      "Epoch 629  train_acc: 0.8630  test_acc: 0.7815  \n",
      "Epoch 630  train_acc: 0.8704  test_acc: 0.7833  \n",
      "Epoch 631  train_acc: 0.8704  test_acc: 0.7824  \n",
      "Epoch 632  train_acc: 0.8778  test_acc: 0.7926  \n",
      "Epoch 633  train_acc: 0.8722  test_acc: 0.7944  \n",
      "Epoch 634  train_acc: 0.8778  test_acc: 0.7843  \n",
      "Epoch 635  train_acc: 0.8806  test_acc: 0.7852  \n",
      "Epoch 636  train_acc: 0.8824  test_acc: 0.7870  \n",
      "Epoch 637  train_acc: 0.8787  test_acc: 0.7843  \n",
      "Epoch 638  train_acc: 0.8898  test_acc: 0.7852  \n",
      "Epoch 639  train_acc: 0.8722  test_acc: 0.7880  \n",
      "Epoch 640  train_acc: 0.8852  test_acc: 0.7861  \n",
      "Epoch 641  train_acc: 0.8667  test_acc: 0.7870  \n",
      "Epoch 642  train_acc: 0.8852  test_acc: 0.7852  \n",
      "Epoch 643  train_acc: 0.8861  test_acc: 0.7880  \n",
      "Epoch 644  train_acc: 0.8759  test_acc: 0.7889  \n",
      "Epoch 645  train_acc: 0.8926  test_acc: 0.7898  \n",
      "Epoch 646  train_acc: 0.8759  test_acc: 0.7907  \n",
      "Epoch 647  train_acc: 0.8685  test_acc: 0.7917  \n",
      "Epoch 648  train_acc: 0.8889  test_acc: 0.7907  \n",
      "Epoch 649  train_acc: 0.8880  test_acc: 0.7898  \n",
      "Epoch 650  train_acc: 0.8769  test_acc: 0.7889  \n",
      "Epoch 651  train_acc: 0.8889  test_acc: 0.7963  \n",
      "Epoch 652  train_acc: 0.8741  test_acc: 0.7944  \n",
      "Epoch 653  train_acc: 0.8880  test_acc: 0.7926  \n",
      "Epoch 654  train_acc: 0.8815  test_acc: 0.7917  \n",
      "Epoch 655  train_acc: 0.8843  test_acc: 0.7917  \n",
      "Epoch 656  train_acc: 0.8833  test_acc: 0.7898  \n",
      "Epoch 657  train_acc: 0.8833  test_acc: 0.7907  \n",
      "Epoch 658  train_acc: 0.8991  test_acc: 0.7944  \n",
      "Epoch 659  train_acc: 0.8806  test_acc: 0.7907  \n",
      "Epoch 660  train_acc: 0.8880  test_acc: 0.7954  \n",
      "Epoch 661  train_acc: 0.8796  test_acc: 0.7944  \n",
      "Epoch 662  train_acc: 0.8861  test_acc: 0.7981  \n",
      "Epoch 663  train_acc: 0.8630  test_acc: 0.7972  \n",
      "Epoch 664  train_acc: 0.8769  test_acc: 0.7944  \n",
      "Epoch 665  train_acc: 0.8815  test_acc: 0.7926  \n",
      "Epoch 666  train_acc: 0.8806  test_acc: 0.7954  \n",
      "Epoch 667  train_acc: 0.8852  test_acc: 0.7963  \n",
      "Epoch 668  train_acc: 0.8907  test_acc: 0.7944  \n",
      "Epoch 669  train_acc: 0.8917  test_acc: 0.7944  \n",
      "Epoch 670  train_acc: 0.8880  test_acc: 0.7917  \n",
      "Epoch 671  train_acc: 0.8824  test_acc: 0.7944  \n",
      "Epoch 672  train_acc: 0.8852  test_acc: 0.7954  \n",
      "Epoch 673  train_acc: 0.8759  test_acc: 0.7954  \n",
      "Epoch 674  train_acc: 0.8861  test_acc: 0.8000  \n",
      "Epoch 675  train_acc: 0.8944  test_acc: 0.7935  \n",
      "Epoch 676  train_acc: 0.8935  test_acc: 0.8000  \n",
      "Epoch 677  train_acc: 0.8806  test_acc: 0.7972  \n",
      "Epoch 678  train_acc: 0.8759  test_acc: 0.7991  \n",
      "Epoch 679  train_acc: 0.8806  test_acc: 0.7991  \n",
      "Epoch 680  train_acc: 0.8954  test_acc: 0.7972  \n",
      "Epoch 681  train_acc: 0.8843  test_acc: 0.7935  \n",
      "Epoch 682  train_acc: 0.8861  test_acc: 0.7972  \n",
      "Epoch 683  train_acc: 0.8981  test_acc: 0.7954  \n",
      "Epoch 684  train_acc: 0.8907  test_acc: 0.7991  \n",
      "Epoch 685  train_acc: 0.8898  test_acc: 0.7963  \n",
      "Epoch 686  train_acc: 0.8676  test_acc: 0.7917  \n",
      "Epoch 687  train_acc: 0.8907  test_acc: 0.7991  \n",
      "Epoch 688  train_acc: 0.9046  test_acc: 0.7972  \n",
      "Epoch 689  train_acc: 0.8861  test_acc: 0.7991  \n",
      "Epoch 690  train_acc: 0.8889  test_acc: 0.7954  \n",
      "Epoch 691  train_acc: 0.8963  test_acc: 0.8028  \n",
      "Epoch 692  train_acc: 0.8861  test_acc: 0.8009  \n",
      "Epoch 693  train_acc: 0.8889  test_acc: 0.8000  \n",
      "Epoch 694  train_acc: 0.8861  test_acc: 0.8019  \n",
      "Epoch 695  train_acc: 0.8907  test_acc: 0.8000  \n",
      "Epoch 696  train_acc: 0.8870  test_acc: 0.7935  \n",
      "Epoch 697  train_acc: 0.8935  test_acc: 0.7991  \n",
      "Epoch 698  train_acc: 0.8806  test_acc: 0.7991  \n",
      "Epoch 699  train_acc: 0.8898  test_acc: 0.8000  \n",
      "Epoch 700  train_acc: 0.8898  test_acc: 0.8019  \n",
      "Save PyTorch Model State to checkpoints/EEGNet|ReLU|lr=0.001|opt=sgd|ep=700model.pth\n",
      "Save learning curves to learning_curves/EEGNet|ReLU|lr=0.001|opt=sgd|ep=700curves.npz\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "hp = {\n",
    "    'bs': 32,\n",
    "    'lr':1e-3,\n",
    "    'epochs':1000,\n",
    "    'optimizer': 'sgd', # optimizer  \n",
    "    'NN':'EEGNet',\n",
    "    'act':'ReLU'\n",
    "}\n",
    "model , hist = train_with_hp(hp,save_result=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-10-d87d6fbfe091>, line 10)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-d87d6fbfe091>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "hp = {\n",
    "    'bs': 64,\n",
    "    'lr':1e-2,\n",
    "    'epochs':300,\n",
    "    'optimizer': 'sgd', # optimizer  \n",
    "    'NN':'EEGNet',\n",
    "    'act':'ReLU'\n",
    "}\n",
    "def read_history(hp):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}